1、	机器学习的一些概念：有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
机器学习：在进行特定编程的情况下，给予计算机学习能力的领域。目前存在几种不同类型的学习算法，主要的两种类型被我们称之为监督学习和无监督学习。，监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。

监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。我们给一系列数据，给定数据集中每个样本的正确答案，即它们实际的答案然后运用学习算法，算出更多的正确答案。
无监督学习。其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。无无监督学习，它是学习策略，交给算法大量的数据，并让算法为我们从数据中找出某种结构。

2、	线性回归的原理
线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。

在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。（这反过来又应当由多个相关的因变量预测的多元线性回归区别，而不是一个单一的标量变量。）
回归分析中有多个自变量：这里有一个原则问题，这些自变量的重要性，究竟谁是最重要，谁是比较重要，谁是不重要。所以，spss线性回归有一个和逐步判别分析的等价的设置。

原理：是F检验。spss中的操作是“分析”～“回归”～“线性”主对话框方法框中需先选定“逐步”方法～“选项”子对话框
如果是选择“用F检验的概率值”，越小代表这个变量越容易进入方程。原因是这个变量的F检验的概率小，说明它显著，也就是这个变量对回归方程的贡献越大，进一步说就是该变量被引入回归方程的资格越大。究其根本，就是零假设分水岭，例如要是把进入设为0.05，大于它说明接受零假设，这个变量对回归方程没有什么重要性，但是一旦小于0.05，说明，这个变量很重要应该引起注意。这个0.05就是进入回归方程的通行证。
下一步：“移除”选项：如果一个自变量F检验的P值也就是概率值大于移除中所设置的值，这个变量就要被移除回归方程。spss回归分析也就是把自变量作为一组待选的商品，高于这个价就不要，低于一个比这个价小一些的就买来。所以“移除”中的值要大于“进入”中的值，默认“进入”值为0.05，“移除”值为0.10
如果，使用“采用F值”作为判据，整个情况就颠倒了，“进入”值大于“移除”值，并且是自变量的进入值需要大于设定值才能进入回归方程。这里的原因就是F检验原理的计算公式。所以才有这样的差别。
结果：如同判别分析的逐步方法，表格中给出所有自变量进入回归方程情况。这个表格的标志是，第一列写着拟合步骤编号，第二列写着每步进入回归方程的编号，第三列写着从回归方程中剔除的自变量。第四列写着自变量引入或者剔除的判据，下面跟着一堆文字。
这种设置的根本目的：挑选符合的变量，剔除不符合的变量。
注意：spss中还有一个设置，“在等式中包含常量”，它的作用是如果不选择它，回归模型经过原点，如果选择它，回归方程就有常数项。这个选项选和不选是不一样的。
在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。
线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其位置参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。
线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚.相反,最小二乘逼近可以用来拟合那些非线性的模型.因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。
回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。
线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。

3、	线性回归损失函数、代价函数、目标函数
损失函数（loss function）或代价函数（cost function）是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。
目标函数f(x)就是用设计变量来表示的所追求的目标形式，所以目标函数就是设计变量的函数，是一个标量。从工程意义讲，目标函数是系统的性能标准，比如，一个结构的最轻重量、最低造价、最合理形式；一件产品的最短生产时间、最小能量消耗；一个实验的最佳配方等等，建立目标函数的过程就是寻找设计变量与目标的关系的过程，目标函数和设计变量的关系可用曲线、曲面或超曲面表示。

4、	优化方法(梯度下降法、牛顿法、拟牛顿法等)
梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。
拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

5、	线性回归的评估指标
Mean dependent var
因变量的样本均值: 目的是为了度量因变量的集中度
E(y)=(y1+y2+,...,+yn)/n
S.D dependent var
因变量的样本标准差: 目的是为了度量因变量的离散度
这里不好编辑公式，省略...
sum squared redis
残差平方和：很多最优化的方法都怡残差平方最小和作为目标函数。越小说明效果越好。
SSR=(e1^2+e2^2+...)
残差平方和会随着回归方程右边变量的增加而减少。
S.E regression
回归标准差：显然是越小越好
 其中,T表示样本的个数，k表示回归方程右式的变量的个数，包括常数项。
log likelihood
和残差一样，可以作为最大似然估计的目标函数，越大越好。
F statistic
检验回归方程的显著性：自变量和因变量的线性关系是否密切。给定显著水平a, 根据自由度(k,n-k-1)查F分布表，
若F>Fa,则显著，否则不显著。以上说的密切关系指的是所有自变量的联合。也就是说至少有一个变量有关，则显著。
F统计量实际上就是检验当删除所有因变量的时候，残差平方和会增加。
 
其中, n表示样本的个数，k表示回归方程右式的变量的个数，包括常数项。
Prob(F-Statistic)
F检验对应的概率，越小越好。
T statistic
判断回归模型右边每个属性是否与因变量关系密切。
同样T>Ta则拒绝原假设。否则该变量可以剔除。
Prob(T-Statistic)
T检验对应的概率，越小越好。
R-squared
R方的取值范围位于[0,1]之间：目的是描述预测y的程度，显然是越大越好，但是也不能因为大就完全认为回归效果好，
还要结合其他的参数，因为R方的值可能因为其他非回归预测效果好的原因导致值变大。
 
其中，分子是残差平方和，分母约等于样本方差。
Adjusted R-squared
目的是为了克服上面所说的因为其他的原因(变量个数增大)导致R方的递增。
 
其中，k是回归方程右边变量的个数 ，包括常数项。所以调整后的R方比R方更可靠。
Durbin-waston stat
 
DW统计量，用于检测误差是否序列相关，如果相关，可以通过预测误差，改进回归模型的效果。
值一般在[0, 4]之间，越接近2，说明不含自相关。
AIC
AIC准则用于预测模型的选择，越小越好
 
其中,T表示样本的个数，k表示回归方程右式的变量的个数，包括常数项。
SIC
和AIC一样，用于预测模型的选择，同样是越小越好
 
其中,T表示样本的个数，k表示回归方程右式的变量的个数，包括常数项。
6、sklearn参数详解
sklearn数据集
sklearn.datasets
load_*  获取小规模数据集
fetch_* 获取大规模数据集
2 sklearn小数据集
sklearn.datasets.load_iris()
3 sklearn大数据集
sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)
4 数据集的返回值
datasets.base.Bunch（继承自字典）
dict["key"] = values
bunch.key = values
